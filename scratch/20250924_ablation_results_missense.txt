Ablation summary [test]
Mode                N      Acc       F1    ROC-AUC     PR-AUC
------------------------------------------------------
cond+prompt      5170   0.8545   0.8160     0.9250     0.9007
cond_only        5170   0.7263   0.5202     0.7777     0.7307
cond_zero_dna    5170   0.8586   0.8206     0.9283     0.9050
cond_zero_prot   5170   0.8139   0.7435     0.8876     0.8474
prompt_only      5170   0.8168   0.7456     0.8894     0.8510
prompt_no_hgvsp   5170   0.8393   0.7907     0.9078     0.8795
prompt_no_hgvsc   5170   0.8542   0.8133     0.9240     0.8997
prompt_no_gene   5170   0.7375   0.6874     0.8146     0.7638
cond+noise       5170   0.7832   0.7461     0.8673     0.8322
cond+permute     5170   0.7793   0.7205     0.8455     0.7617
pure-noise       5170   0.7797   0.7405     0.8662     0.8238
cond×scale=0     5170   0.8168   0.7456     0.8894     0.8510
cond×scale=0.5   5170   0.8426   0.7923     0.9136     0.8849
cond×scale=1     5170   0.8545   0.8160     0.9250     0.9007
cond×scale=2     5170   0.8431   0.8086     0.9206     0.8902

Δ vs cond+prompt:
- cond_only     ΔAcc=-0.1282  ΔF1=-0.2958  ΔROC-AUC=-0.1473  ΔPR-AUC=-0.1701
- cond_zero_dna  ΔAcc=+0.0041  ΔF1=+0.0047  ΔROC-AUC=+0.0033  ΔPR-AUC=+0.0042
- cond_zero_prot  ΔAcc=-0.0406  ΔF1=-0.0725  ΔROC-AUC=-0.0374  ΔPR-AUC=-0.0533
- prompt_only   ΔAcc=-0.0377  ΔF1=-0.0703  ΔROC-AUC=-0.0355  ΔPR-AUC=-0.0497
- prompt_no_hgvsp  ΔAcc=-0.0153  ΔF1=-0.0252  ΔROC-AUC=-0.0172  ΔPR-AUC=-0.0212
- prompt_no_hgvsc  ΔAcc=-0.0004  ΔF1=-0.0027  ΔROC-AUC=-0.0010  ΔPR-AUC=-0.0010
- prompt_no_gene  ΔAcc=-0.1170  ΔF1=-0.1286  ΔROC-AUC=-0.1103  ΔPR-AUC=-0.1370
- cond+noise    ΔAcc=-0.0714  ΔF1=-0.0699  ΔROC-AUC=-0.0577  ΔPR-AUC=-0.0685
- cond+permute  ΔAcc=-0.0752  ΔF1=-0.0954  ΔROC-AUC=-0.0795  ΔPR-AUC=-0.1390
- pure-noise    ΔAcc=-0.0749  ΔF1=-0.0755  ΔROC-AUC=-0.0588  ΔPR-AUC=-0.0769
- cond×scale=0  ΔAcc=-0.0377  ΔF1=-0.0703  ΔROC-AUC=-0.0355  ΔPR-AUC=-0.0497
- cond×scale=0.5  ΔAcc=-0.0120  ΔF1=-0.0236  ΔROC-AUC=-0.0113  ΔPR-AUC=-0.0158
- cond×scale=1  ΔAcc=+0.0000  ΔF1=+0.0000  ΔROC-AUC=+0.0000  ΔPR-AUC=+0.0000
- cond×scale=2  ΔAcc=-0.0114  ΔF1=-0.0074  ΔROC-AUC=-0.0044  ΔPR-AUC=-0.0105


{'cond_only': {'split': 'test',
  'n': 5170,
  'accuracy': 0.7263056092843327,
  'precision': 0.8560267857142857,
  'recall': 0.3735996103263517,
  'f1': 0.5201763309596473,
  'auc_roc': 0.7777241252462612,
  'auc_pr': 0.7306887136513726,
  'confusion_matrix': [[2988, 129], [1286, 767]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.699     0.959     0.809      3117\n  Pathogenic      0.856     0.374     0.520      2053\n\n    accuracy                          0.726      5170\n   macro avg      0.778     0.666     0.664      5170\nweighted avg      0.761     0.726     0.694      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.20175424218177795,
     'Pathogenic': 0.7982457280158997}},
   {'idx': 1,
    'truth': 0,
    'pred': 1,
    'probs': {'Benign': 0.2845262885093689, 'Pathogenic': 0.7154737114906311}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.09274939447641373,
     'Pathogenic': 0.9072506427764893}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9334649443626404,
     'Pathogenic': 0.06653506308794022}},
   {'idx': 4,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5273165702819824,
     'Pathogenic': 0.47268348932266235}},
   {'idx': 5,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.891828715801239, 'Pathogenic': 0.10817128419876099}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.19189491868019104,
     'Pathogenic': 0.8081050515174866}},
   {'idx': 7,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5506073236465454, 'Pathogenic': 0.4493926167488098}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.15613704919815063,
     'Pathogenic': 0.8438629508018494}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.11911322176456451,
     'Pathogenic': 0.8808867335319519}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'cond+prompt': {'split': 'test',
  'n': 5170,
  'accuracy': 0.8545454545454545,
  'precision': 0.8199704869650762,
  'recall': 0.8119824646858256,
  'f1': 0.8159569260890847,
  'auc_roc': 0.9249796185492534,
  'auc_pr': 0.9007497364104713,
  'confusion_matrix': [[2751, 366], [386, 1667]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.877     0.883     0.880      3117\n  Pathogenic      0.820     0.812     0.816      2053\n\n    accuracy                          0.855      5170\n   macro avg      0.848     0.847     0.848      5170\nweighted avg      0.854     0.855     0.854      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.18473070859909058,
     'Pathogenic': 0.8152692914009094}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.8962116241455078,
     'Pathogenic': 0.10378837585449219}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.03511691838502884,
     'Pathogenic': 0.9648831486701965}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9796761274337769,
     'Pathogenic': 0.020323842763900757}},
   {'idx': 4,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.3557189106941223, 'Pathogenic': 0.6442810297012329}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.1755203902721405, 'Pathogenic': 0.8244795799255371}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.07362963259220123,
     'Pathogenic': 0.9263703227043152}},
   {'idx': 7,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.29088014364242554,
     'Pathogenic': 0.7091198563575745}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.06957334280014038,
     'Pathogenic': 0.9304266571998596}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.05109013617038727,
     'Pathogenic': 0.9489098191261292}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'cond_zero_dna': {'split': 'test',
  'n': 5170,
  'accuracy': 0.8586073500967119,
  'precision': 0.8269040553907022,
  'recall': 0.8144179249878227,
  'f1': 0.8206134969325153,
  'auc_roc': 0.9283229265653634,
  'auc_pr': 0.9049785177550457,
  'confusion_matrix': [[2767, 350], [381, 1672]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.879     0.888     0.883      3117\n  Pathogenic      0.827     0.814     0.821      2053\n\n    accuracy                          0.859      5170\n   macro avg      0.853     0.851     0.852      5170\nweighted avg      0.858     0.859     0.858      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.14036570489406586, 'Pathogenic': 0.859634280204773}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.9005680084228516,
     'Pathogenic': 0.09943193197250366}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.03311528265476227,
     'Pathogenic': 0.9668846726417542}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9762900471687317,
     'Pathogenic': 0.023709945380687714}},
   {'idx': 4,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.32423537969589233,
     'Pathogenic': 0.6757646203041077}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.18718840181827545, 'Pathogenic': 0.812811553478241}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.19195173680782318,
     'Pathogenic': 0.8080483078956604}},
   {'idx': 7,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.2628891170024872, 'Pathogenic': 0.7371108531951904}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.18017879128456116,
     'Pathogenic': 0.8198211789131165}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.0378924235701561,
     'Pathogenic': 0.9621075391769409}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'cond_zero_prot': {'split': 'test',
  'n': 5170,
  'accuracy': 0.8139264990328821,
  'precision': 0.8214496169711255,
  'recall': 0.6790063321967852,
  'f1': 0.7434666666666667,
  'auc_roc': 0.8875611970931997,
  'auc_pr': 0.847404015783317,
  'confusion_matrix': [[2814, 303], [659, 1394]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.810     0.903     0.854      3117\n  Pathogenic      0.821     0.679     0.743      2053\n\n    accuracy                          0.814      5170\n   macro avg      0.816     0.791     0.799      5170\nweighted avg      0.815     0.814     0.810      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.7249675989151001, 'Pathogenic': 0.2750323414802551}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.9770246148109436,
     'Pathogenic': 0.022975312545895576}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.44167304039001465,
     'Pathogenic': 0.5583270192146301}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9539446234703064,
     'Pathogenic': 0.046055346727371216}},
   {'idx': 4,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6654649376869202, 'Pathogenic': 0.3345350921154022}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.10666151344776154,
     'Pathogenic': 0.8933385610580444}},
   {'idx': 6,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.554470419883728, 'Pathogenic': 0.4455295503139496}},
   {'idx': 7,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.7773420810699463,
     'Pathogenic': 0.22265788912773132}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.4493926167488098, 'Pathogenic': 0.5506073236465454}},
   {'idx': 9,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.709019124507904,
     'Pathogenic': 0.29098087549209595}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'prompt_only': {'split': 'test',
  'n': 5170,
  'accuracy': 0.8168278529980658,
  'precision': 0.8311377245508982,
  'recall': 0.6760837798343887,
  'f1': 0.7456352403975288,
  'auc_roc': 0.8894378376300417,
  'auc_pr': 0.8510387915660097,
  'confusion_matrix': [[2835, 282], [665, 1388]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.810     0.910     0.857      3117\n  Pathogenic      0.831     0.676     0.746      2053\n\n    accuracy                          0.817      5170\n   macro avg      0.821     0.793     0.801      5170\nweighted avg      0.818     0.817     0.813      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6992030739784241,
     'Pathogenic': 0.30079689621925354}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.9728227853775024,
     'Pathogenic': 0.027177300304174423}},
   {'idx': 2,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5888891220092773,
     'Pathogenic': 0.41111084818840027}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9518296718597412,
     'Pathogenic': 0.04817034304141998}},
   {'idx': 4,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6548395752906799,
     'Pathogenic': 0.34516045451164246}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.11442418396472931,
     'Pathogenic': 0.8855757713317871}},
   {'idx': 6,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5195212960243225, 'Pathogenic': 0.4804786443710327}},
   {'idx': 7,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6406920552253723, 'Pathogenic': 0.3593078851699829}},
   {'idx': 8,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5195212960243225, 'Pathogenic': 0.4804786443710327}},
   {'idx': 9,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6476868391036987,
     'Pathogenic': 0.35231319069862366}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'prompt_no_hgvsp': {'split': 'test',
  'n': 5170,
  'accuracy': 0.8392649903288201,
  'precision': 0.8185610010427529,
  'recall': 0.7647345348270823,
  'f1': 0.7907328128934777,
  'auc_roc': 0.9078048962675184,
  'auc_pr': 0.8795418994430325,
  'confusion_matrix': [[2769, 348], [483, 1570]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.851     0.888     0.870      3117\n  Pathogenic      0.819     0.765     0.791      2053\n\n    accuracy                          0.839      5170\n   macro avg      0.835     0.827     0.830      5170\nweighted avg      0.838     0.839     0.838      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.22278465330600739,
     'Pathogenic': 0.7772152423858643}},
   {'idx': 1,
    'truth': 0,
    'pred': 1,
    'probs': {'Benign': 0.3209277093410492, 'Pathogenic': 0.6790723204612732}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.10820074379444122, 'Pathogenic': 0.89179927110672}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.984812319278717,
     'Pathogenic': 0.015187680721282959}},
   {'idx': 4,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.4455295503139496, 'Pathogenic': 0.554470419883728}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.3175320625305176, 'Pathogenic': 0.6824679374694824}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.10382241755723953,
     'Pathogenic': 0.8961775898933411}},
   {'idx': 7,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.2720728814601898, 'Pathogenic': 0.727927029132843}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.0952863097190857, 'Pathogenic': 0.9047136902809143}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.04669778421521187,
     'Pathogenic': 0.9533022046089172}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'prompt_no_hgvsc': {'split': 'test',
  'n': 5170,
  'accuracy': 0.8541586073500967,
  'precision': 0.8272040302267003,
  'recall': 0.7998051631758403,
  'f1': 0.8132738979692917,
  'auc_roc': 0.9240087942229037,
  'auc_pr': 0.8997459654881661,
  'confusion_matrix': [[2774, 343], [411, 1642]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.871     0.890     0.880      3117\n  Pathogenic      0.827     0.800     0.813      2053\n\n    accuracy                          0.854      5170\n   macro avg      0.849     0.845     0.847      5170\nweighted avg      0.854     0.854     0.854      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.2044212967157364, 'Pathogenic': 0.79557865858078}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.9005680084228516,
     'Pathogenic': 0.09943193197250366}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.04885069280862808,
     'Pathogenic': 0.9511492848396301}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9814623594284058,
     'Pathogenic': 0.01853768154978752}},
   {'idx': 4,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.4148988425731659, 'Pathogenic': 0.5851011276245117}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.1800345480442047, 'Pathogenic': 0.8199654817581177}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.09398937970399857, 'Pathogenic': 0.906010627746582}},
   {'idx': 7,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.4455295503139496, 'Pathogenic': 0.554470419883728}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.09144799411296844,
     'Pathogenic': 0.9085520505905151}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.08759798854589462,
     'Pathogenic': 0.9124020338058472}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'prompt_no_gene': {'split': 'test',
  'n': 5170,
  'accuracy': 0.7375241779497098,
  'precision': 0.6520979020979021,
  'recall': 0.7267413541159279,
  'f1': 0.6873992167703294,
  'auc_roc': 0.8146316235417514,
  'auc_pr': 0.7637501604489952,
  'confusion_matrix': [[2321, 796], [561, 1492]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.805     0.745     0.774      3117\n  Pathogenic      0.652     0.727     0.687      2053\n\n    accuracy                          0.738      5170\n   macro avg      0.729     0.736     0.731      5170\nweighted avg      0.744     0.738     0.739      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.09538106620311737,
     'Pathogenic': 0.9046189188957214}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.935309648513794, 'Pathogenic': 0.06469041854143143}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.004835501313209534,
     'Pathogenic': 0.9951645135879517}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6261813640594482,
     'Pathogenic': 0.37381866574287415}},
   {'idx': 4,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.17777517437934875,
     'Pathogenic': 0.8222247958183289}},
   {'idx': 5,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5736784338951111, 'Pathogenic': 0.4263215959072113}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.07813090831041336,
     'Pathogenic': 0.9218690991401672}},
   {'idx': 7,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.3557748794555664, 'Pathogenic': 0.6442251205444336}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.06565723568201065,
     'Pathogenic': 0.9343427419662476}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.08153346925973892,
     'Pathogenic': 0.9184665679931641}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'cond+noise': {'split': 'test',
  'n': 5170,
  'accuracy': 0.7831721470019343,
  'precision': 0.6972904318374259,
  'recall': 0.8022406234778373,
  'f1': 0.7460928652321631,
  'auc_roc': 0.8673160133585427,
  'auc_pr': 0.832234795184984,
  'confusion_matrix': [[2402, 715], [406, 1647]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.855     0.771     0.811      3117\n  Pathogenic      0.697     0.802     0.746      2053\n\n    accuracy                          0.783      5170\n   macro avg      0.776     0.786     0.778      5170\nweighted avg      0.793     0.783     0.785      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.47268348932266235,
     'Pathogenic': 0.5273165702819824}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.9399323463439941,
     'Pathogenic': 0.06006769835948944}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.33802247047424316,
     'Pathogenic': 0.6619774699211121}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9766573905944824,
     'Pathogenic': 0.023342592641711235}},
   {'idx': 4,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.1755557507276535, 'Pathogenic': 0.8244442343711853}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.012442144565284252,
     'Pathogenic': 0.9875578880310059}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.32423537969589233,
     'Pathogenic': 0.6757646203041077}},
   {'idx': 7,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5117166042327881, 'Pathogenic': 0.4882833957672119}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.1801607310771942, 'Pathogenic': 0.8198392391204834}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.3073579967021942,
     'Pathogenic': 0.6926419734954834}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'cond+permute': {'split': 'test',
  'n': 5170,
  'accuracy': 0.7793036750483558,
  'precision': 0.7246305418719212,
  'recall': 0.7165124208475402,
  'f1': 0.7205486162135685,
  'auc_roc': 0.8454933670625442,
  'auc_pr': 0.7617179711088393,
  'confusion_matrix': [[2558, 559], [582, 1471]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.815     0.821     0.818      3117\n  Pathogenic      0.725     0.717     0.721      2053\n\n    accuracy                          0.779      5170\n   macro avg      0.770     0.769     0.769      5170\nweighted avg      0.779     0.779     0.779      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.654949963092804, 'Pathogenic': 0.34505006670951843}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.9808785915374756,
     'Pathogenic': 0.019121458753943443}},
   {'idx': 2,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6334669589996338, 'Pathogenic': 0.3665330111980438}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9481155276298523, 'Pathogenic': 0.0518844872713089}},
   {'idx': 4,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.39981159567832947,
     'Pathogenic': 0.6001883745193481}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.0675351619720459, 'Pathogenic': 0.9324648380279541}},
   {'idx': 6,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5467381477355957, 'Pathogenic': 0.4532618820667267}},
   {'idx': 7,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5736784338951111, 'Pathogenic': 0.4263215959072113}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.45713672041893005,
     'Pathogenic': 0.5428632497787476}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.03511587902903557,
     'Pathogenic': 0.9648841023445129}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'pure-noise': {'split': 'test',
  'n': 5170,
  'accuracy': 0.7796905222437137,
  'precision': 0.6956335616438356,
  'recall': 0.7915245981490502,
  'f1': 0.7404875825928458,
  'auc_roc': 0.8661789182743284,
  'auc_pr': 0.8238198536095317,
  'confusion_matrix': [[2406, 711], [428, 1625]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.849     0.772     0.809      3117\n  Pathogenic      0.696     0.792     0.740      2053\n\n    accuracy                          0.780      5170\n   macro avg      0.772     0.782     0.775      5170\nweighted avg      0.788     0.780     0.782      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.4804786443710327, 'Pathogenic': 0.5195212960243225}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.9149484038352966,
     'Pathogenic': 0.08505154401063919}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.24794189631938934,
     'Pathogenic': 0.7520580291748047}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.8289907574653625,
     'Pathogenic': 0.17100928723812103}},
   {'idx': 4,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.20173457264900208,
     'Pathogenic': 0.7982653379440308}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.019121747463941574,
     'Pathogenic': 0.9808782339096069}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.4035668671131134, 'Pathogenic': 0.5964331030845642}},
   {'idx': 7,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.626067042350769, 'Pathogenic': 0.37393298745155334}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.3849121332168579, 'Pathogenic': 0.6150878667831421}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.11119293421506882,
     'Pathogenic': 0.8888071179389954}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'cond×scale=0': {'split': 'test',
  'n': 5170,
  'accuracy': 0.8168278529980658,
  'precision': 0.8311377245508982,
  'recall': 0.6760837798343887,
  'f1': 0.7456352403975288,
  'auc_roc': 0.8894378376300417,
  'auc_pr': 0.8510387915660097,
  'confusion_matrix': [[2835, 282], [665, 1388]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.810     0.910     0.857      3117\n  Pathogenic      0.831     0.676     0.746      2053\n\n    accuracy                          0.817      5170\n   macro avg      0.821     0.793     0.801      5170\nweighted avg      0.818     0.817     0.813      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6992030739784241,
     'Pathogenic': 0.30079689621925354}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.9728227853775024,
     'Pathogenic': 0.027177300304174423}},
   {'idx': 2,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5888891220092773,
     'Pathogenic': 0.41111084818840027}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9518296718597412,
     'Pathogenic': 0.04817034304141998}},
   {'idx': 4,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6548395752906799,
     'Pathogenic': 0.34516045451164246}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.11442418396472931,
     'Pathogenic': 0.8855757713317871}},
   {'idx': 6,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5195212960243225, 'Pathogenic': 0.4804786443710327}},
   {'idx': 7,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6406920552253723, 'Pathogenic': 0.3593078851699829}},
   {'idx': 8,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5195212960243225, 'Pathogenic': 0.4804786443710327}},
   {'idx': 9,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.6476868391036987,
     'Pathogenic': 0.35231319069862366}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'cond×scale=0.5': {'split': 'test',
  'n': 5170,
  'accuracy': 0.8425531914893617,
  'precision': 0.8318157471880021,
  'recall': 0.7564539698002922,
  'f1': 0.7923469387755102,
  'auc_roc': 0.9136297640908608,
  'auc_pr': 0.8849431748873876,
  'confusion_matrix': [[2803, 314], [500, 1553]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.849     0.899     0.873      3117\n  Pathogenic      0.832     0.756     0.792      2053\n\n    accuracy                          0.843      5170\n   macro avg      0.840     0.828     0.833      5170\nweighted avg      0.842     0.843     0.841      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.4148988425731659, 'Pathogenic': 0.5851011276245117}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.9643435478210449,
     'Pathogenic': 0.03565650433301926}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.1778644174337387, 'Pathogenic': 0.8221355676651001}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9643382430076599,
     'Pathogenic': 0.03566175699234009}},
   {'idx': 4,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.5039061903953552, 'Pathogenic': 0.4960938096046448}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.14033624529838562,
     'Pathogenic': 0.8596637845039368}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.34521564841270447,
     'Pathogenic': 0.6547843813896179}},
   {'idx': 7,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.49218812584877014,
     'Pathogenic': 0.5078119039535522}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.26894140243530273,
     'Pathogenic': 0.7310585379600525}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.36291274428367615,
     'Pathogenic': 0.6370872259140015}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'cond×scale=1': {'split': 'test',
  'n': 5170,
  'accuracy': 0.8545454545454545,
  'precision': 0.8199704869650762,
  'recall': 0.8119824646858256,
  'f1': 0.8159569260890847,
  'auc_roc': 0.9249796185492534,
  'auc_pr': 0.9007497364104713,
  'confusion_matrix': [[2751, 366], [386, 1667]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.877     0.883     0.880      3117\n  Pathogenic      0.820     0.812     0.816      2053\n\n    accuracy                          0.855      5170\n   macro avg      0.848     0.847     0.848      5170\nweighted avg      0.854     0.855     0.854      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.18473070859909058,
     'Pathogenic': 0.8152692914009094}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.8962116241455078,
     'Pathogenic': 0.10378837585449219}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.03511691838502884,
     'Pathogenic': 0.9648831486701965}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.9796761274337769,
     'Pathogenic': 0.020323842763900757}},
   {'idx': 4,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.3557189106941223, 'Pathogenic': 0.6442810297012329}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.1755203902721405, 'Pathogenic': 0.8244795799255371}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.07362963259220123,
     'Pathogenic': 0.9263703227043152}},
   {'idx': 7,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.29088014364242554,
     'Pathogenic': 0.7091198563575745}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.06957334280014038,
     'Pathogenic': 0.9304266571998596}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.05109013617038727,
     'Pathogenic': 0.9489098191261292}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'},
 'cond×scale=2': {'split': 'test',
  'n': 5170,
  'accuracy': 0.8431334622823985,
  'precision': 0.7843406593406593,
  'recall': 0.8343886994641987,
  'f1': 0.8085909841869248,
  'auc_roc': 0.920578522224884,
  'auc_pr': 0.890240068700465,
  'confusion_matrix': [[2646, 471], [340, 1713]],
  'report': '              precision    recall  f1-score   support\n\n      Benign      0.886     0.849     0.867      3117\n  Pathogenic      0.784     0.834     0.809      2053\n\n    accuracy                          0.843      5170\n   macro avg      0.835     0.842     0.838      5170\nweighted avg      0.846     0.843     0.844      5170\n',
  'examples': [{'idx': 0,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.10823606699705124,
     'Pathogenic': 0.8917639255523682}},
   {'idx': 1,
    'truth': 0,
    'pred': 0,
    'probs': {'Benign': 0.8634060025215149, 'Pathogenic': 0.1365939974784851}},
   {'idx': 2,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.054184652864933014,
     'Pathogenic': 0.9458153247833252}},
   {'idx': 3,
    'truth': 1,
    'pred': 0,
    'probs': {'Benign': 0.8932744860649109,
     'Pathogenic': 0.10672549903392792}},
   {'idx': 4,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.10085649788379669,
     'Pathogenic': 0.8991435170173645}},
   {'idx': 5,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.1422765702009201, 'Pathogenic': 0.8577234148979187}},
   {'idx': 6,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.03259936720132828,
     'Pathogenic': 0.9674006700515747}},
   {'idx': 7,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.09131623804569244, 'Pathogenic': 0.908683717250824}},
   {'idx': 8,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.044657427817583084,
     'Pathogenic': 0.9553425908088684}},
   {'idx': 9,
    'truth': 1,
    'pred': 1,
    'probs': {'Benign': 0.025586888194084167,
     'Pathogenic': 0.9744130969047546}}],
  'predictions_path': 'artifacts/qwen3/test_scored.feather'}}

